import gradio as gr
from inference import inference_model

############################### PRICE PREDICTION ###############################
################################################################################
# H√†m d·ª± ƒëo√°n gi√°
def predict_price(property_type, area, floors, bedrooms, toilets, legal_status, furniture, project_name, district, distance):
    price = inference_model(property_type, area, floors, bedrooms, toilets, legal_status, furniture, project_name, district, distance)
    return f"{price:,.2f} t·ª∑ ƒë·ªìng"

# Giao di·ªán trang ch·ªß
def home():
    with gr.Blocks() as demo:
        with gr.Column():
            gr.Markdown("""
            <div style="text-align: center;">
                <h1>Ch√†o m·ª´ng ƒë·∫øn v·ªõi ·ª©ng d·ª•ng H·ªá th·ªëng B·∫•t ƒë·ªông s·∫£n $mart üè†!</h1>
                <p>Ch√∫ng t√¥i cung c·∫•p c√¥ng c·ª• d·ª± ƒëo√°n gi√° b·∫•t ƒë·ªông s·∫£n ch√≠nh x√°c d·ª±a tr√™n c√°c tham s·ªë ƒë·∫ßu v√†o v√† t∆∞ v·∫•n qua chatbot.</p>
            </div>
            """)
            gr.Image("assets/pexels-binyaminmellish-106399.jpg", elem_id="real-estate-image", show_label=False, interactive=False)
        
            gr.Markdown("""
            <div style="text-align: center;">
                <h2>T·∫°i sao l·∫°i ch·ªçn c√¥ng c·ª• d·ª± ƒëo√°n c·ªßa ch√∫ng t√¥i?</h2>
                <ul style="list-style-type:none; text-align: left; display: inline-block;">
                    <li>‚úîÔ∏è D·ª± ƒëo√°n ch√≠nh x√°c d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø</li>
                    <li>‚úîÔ∏è Giao di·ªán d·ªÖ s·ª≠ d·ª•ng</li>
                    <li>‚úîÔ∏è Nhanh ch√≥ng v√† ƒë√°ng tin c·∫≠y</li>
                </ul>
                <h2>C√°ch s·ª≠ d·ª•ng giao di·ªán d·ª± ƒëo√°n</h2>
                <p>Th·ª±c hi·ªán theo c√°c b∆∞·ªõc ƒë∆°n gi·∫£n sau ƒë·ªÉ nh·∫≠n d·ª± ƒëo√°n gi√° b·∫•t ƒë·ªông s·∫£n c·ªßa b·∫°n:</p>
                <ol style="text-align: left; display: inline-block;">
                    <li>Nh·∫•p v√†o tab 'D·ª± ƒëo√°n gi√°'</li>
                    <li>Nh·∫≠p th√¥ng tin chi ti·∫øt v·ªÅ c√°c thu·ªôc t√≠nh b·∫•t ƒë·ªông s·∫£n b·∫°n c·∫ßn h·ªèi ƒë√°p</li>
                    <li>Nh·∫•p v√†o n√∫t 'D·ª± ƒëo√°n'</li>
                    <li>Nh·∫≠n gi√° d·ª± ƒëo√°n ngay l·∫≠p t·ª©c</li>
                </ol>
                <h2>H·ªó tr·ª£ chatbot</h2>
                <p>‚úîÔ∏è N·∫øu b·∫°n c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o ho·∫∑c c·∫ßn h·ªó tr·ª£ th√™m, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng chatbot c·ªßa ch√∫ng t√¥i ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ v√† t∆∞ v·∫•n ngay l·∫≠p t·ª©c.</p>
                <h2>C√°ch s·ª≠ d·ª•ng chatbot</h2>
                <p>Th·ª±c hi·ªán theo c√°c b∆∞·ªõc ƒë∆°n gi·∫£n sau ƒë·ªÉ s·ª≠ d·ª•ng chatbot:</p>
                <ol style="text-align: left; display: inline-block;">
                    <li>Nh·∫•p v√†o tab 'Chatbot'</li>
                    <li>Nh·∫≠p th√¥ng tin b·∫°n c·∫ßn h·ªèi</li>
                    <li>Nh·∫≠n th√¥ng tin t·ª´ chatbot</li>
                    <li>Sau ƒë√≥ b·∫°n c√≥ th·ªÉ tr√≤ chuy·ªán v·ªõi chatbot</li>
                </ol>               
                <h2>V·ªÅ ch√∫ng t√¥i</h2>
                <p>Nh√≥m ch√∫ng t√¥i bao g·ªìm c√°c sinh vi√™n ƒë·∫øn t·ª´ Tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± Nhi√™n - ƒê·∫°i h·ªçc Qu·ªëc gia TP.HCM.</p>
                <p>ƒê√¢y l√† s·∫£n ph·∫©m thu·ªôc h·ªçc ph·∫ßn ƒê·ªì √°n t·ªët nghi·ªáp do nh√≥m ch√∫ng t√¥i nghi√™n c·ª©u v√† s·∫£n xu·∫•t.</p>
                <h2>Li√™n h·ªá v·ªõi ch√∫ng t√¥i</h2>
                <p>N·∫øu b·∫°n c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o ho·∫∑c c·∫ßn h·ªó tr·ª£ th√™m, ƒë·ª´ng ng·∫ßn ng·∫°i li√™n h·ªá v·ªõi ch√∫ng t√¥i th√¥ng qua email n√†y: <a href="mailto:20280047@student.hcmus.edu.vn"></a>.</p>
            </div>
            """)
    return demo

# Giao di·ªán d·ª± ƒëo√°n
def predict_interface():
    with gr.Blocks() as demo:
        gr.Markdown("""
        <style>
            body {font-family: Arial, sans-serif;}
            h1 {color: #4CAF50;}
            .gradio-container {background-color: #f9f9f9; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);}
            .gradio-row {margin-bottom: 10px;}
        </style>
        """)
        gr.Markdown("<h1 style='text-align: center;'>üí∞ Real Estate Price Prediction</h1>")
        gr.Image("assets/pexels-binyaminmellish-1396122.jpg", elem_id="real-estate-image", show_label=False, interactive=False)

        with gr.Row():
            with gr.Column():
                property_type_input = gr.Dropdown(
                    label="üè† Lo·∫°i h√¨nh b·∫•t ƒë·ªông s·∫£n",
                    choices=["Chung c∆∞", "Nh√† ng√µ, h·∫ªm", "Nh√† m·∫∑t ph·ªë", "Nh√† bi·ªát th·ª±", "Nh√† ph·ªë li·ªÅn k·ªÅ"],
                    value="Chung c∆∞"
                )
                district_input = gr.Dropdown(
                    label="üìç Qu·∫≠n/Huy·ªán",
                    choices=["Qu·∫≠n 1", "Qu·∫≠n 3", "Qu·∫≠n 4", "Qu·∫≠n 5", "Qu·∫≠n 6", "Qu·∫≠n 7", "Qu·∫≠n 8", "Qu·∫≠n 10", "Qu·∫≠n 11", "Qu·∫≠n 12", "B√¨nh Ch√°nh", "B√¨nh T√¢n", "B√¨nh Th·∫°nh", "C·∫ßn Gi·ªù", "C·ªß Chi", "H√≥c M√¥n", "Nh√† B√®", "Ph√∫ Nhu·∫≠n", "G√≤ V·∫•p", "T√¢n B√¨nh", "T√¢n Ph√∫", "TP Th·ªß ƒê·ª©c"],
                    value="TP Th·ªß ƒê·ª©c"
                )
                distance_input = gr.Slider(
                    label="üõ£Ô∏è Kho·∫£ng c√°ch ƒë·∫øn trung t√¢m th√†nh ph·ªë (ch·ª£ B·∫øn Th√†nh) (km)",
                    minimum=0.1, maximum=200, step=0.1, value=50
                )
                area_input = gr.Slider(
                    label="üìè Di·ªán t√≠ch (m¬≤)",
                    minimum=7, maximum=170, step=0.1, value=50
                )
                legal_status_input = gr.Dropdown(
                    label="üìú T√¨nh tr·∫°ng ph√°p l√Ω",
                    choices=["ƒê√£ c√≥ s·ªï", "Gi·∫•y t·ªù kh√°c", "ƒêang ch·ªù s·ªï", "Unknown"],
                    value="ƒê√£ c√≥ s·ªï"
                )
            with gr.Column():
                project_name_input = gr.Textbox(
                    label="üìù T√™n d·ª± √°n",
                    value="Unknown"
                )
                furniture_input = gr.Dropdown(
                    label="üõãÔ∏è N·ªôi th·∫•t",
                    choices=["Kh√¥ng c√≥ n·ªôi th·∫•t", "C√≥ n·ªôi th·∫•t"],
                    value="Kh√¥ng c√≥ n·ªôi th·∫•t"
                )
                floors_input = gr.Slider(
                    label="üè¢ S·ªë t·∫ßng",
                    minimum=1, maximum=30, step=1, value=1
                )
                bedrooms_input = gr.Slider(
                    label="üõèÔ∏è S·ªë ph√≤ng ng·ªß",
                    minimum=1, maximum=60, step=1, value=1
                )
                toilets_input = gr.Slider(
                    label="üöΩ S·ªë toilets",
                    minimum=1, maximum=7, step=1, value=1
                )
            
        predict_btn = gr.Button("D·ª± ƒëo√°n", variant="primary")
        result = gr.Textbox(label="üí∞ Gi√° d·ª± ƒëo√°n")
        
        predict_btn.click(predict_price, 
                          inputs=[property_type_input, area_input,
                                  floors_input, bedrooms_input, toilets_input,
                                  legal_status_input, furniture_input, project_name_input, 
                                  district_input, distance_input],
                          outputs=result)
    
    return demo


############################### CHATBOT ###############################
#######################################################################
import gradio as gr
import os
import codecs
import openai
import time
from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer
from pyvi.ViTokenizer import tokenize
#from dotenv import load_dotenv

#load_dotenv()

pc = Pinecone(api_key="os.getenv()")
cloud = os.environ.get("PINECONE_CLOUD") or "aws"
region = os.environ.get("PINECONE_REGION") or "us-east-1"

spec = ServerlessSpec(cloud=cloud, region=region)

# Set up OpenAI API
client = openai.OpenAI(
    # This is the default and can be omitted
    api_key="os.getenv()")
# Load the Vietnamese embedding model
model = SentenceTransformer('intfloat/multilingual-e5-small')

# Function to get embeddings


def get_embeddings(sentences):
    tokenizer_sent = [tokenize(sent) for sent in sentences]
    embeddings = model.encode(tokenizer_sent)
    return embeddings


# Function to create metadata


def create_meta_batch(sentences):
    return [{"text": sent} for sent in sentences]


# Function to read sentences from a file


def read_sentences_from_file(file_path): # chunk
    with open(file_path, "r", encoding="utf-8-sig") as file:
        sentences = file.readlines()
    # Remove any leading/trailing whitespace
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    return sentences


def process_and_upsert(sentences, batch_size=100, index_name="rag-index"):
    print(pc.list_indexes().names())
    if index_name not in pc.list_indexes().names():
        # if does not exist, create index
        pc.create_index(
            index_name,
            dimension=384,  # dimensionality of text-embedding-ada-002
            metric="cosine",
            spec=spec,
        )

        # connect to index
        index = pc.Index(index_name)

        for i in range(0, len(sentences), batch_size):
            batch = sentences[i : i + batch_size]
            embeddings = get_embeddings(batch)
            ids = [str(i + j) for j in range(len(batch))]
            meta_batch = create_meta_batch(batch)
            vectors = [
                {"id": id_, "values": embedding.tolist(), "metadata": meta}
                for id_, embedding, meta in zip(ids, embeddings, meta_batch)
            ]
            index.upsert(vectors)
    else:
        index = pc.Index(index_name)
    return index



# Initialize vector database
file_path = "./content/data.txt"
sentences = read_sentences_from_file(file_path)
index = process_and_upsert(sentences, batch_size=100, index_name="rag-index")

# A dictionary to store conversation history for each session
conversation_history = {}

# Define the system prompt with rules
system_prompt = """
You are a chatbot designed to act as a real estate consulting assistant. Your main role is to provide users with accurate and useful information, advice, and insights on real estate queries. When answering questions, you must adhere to the following principles:

1. Accuracy and Relevance: Ensure your answers are based on current and relevant real estate data and trends, but do not reference or refer to specific data sources in your responses.
2. Scope Management: If a query is beyond your capabilities or tools, instruct users on how to find additional help or suggest alternative methods to find the information they require.
3. Scope Management: If a query is beyond your capabilities or tools, instruct users on how to find additional help or suggest alternative methods to find the information they require.
4. Information Gathering: Ask for additional information if it is necessary to provide a more accurate answer.
5. Language Consistency: Always reply in the user's language. If the user speaks Vietnamese, you should reply in Vietnamese.

I. Identifying User Needs:
1. Ask the user what type of real estate service they are interested in (buying, selling, renting, or investing).
2. Gather basic information about their requirements (location, budget, property type, etc.).

II. Providing Information and Options
1. Based on the user‚Äôs requirements, provide information on available properties or services.
2. Share links or details of properties that match their criteria.
3. Offer to schedule viewings or consultations if applicable.

III. Answering Questions
1. Be prepared to answer common questions related to real estate transactions (e.g., mortgage rates, property taxes, neighborhood details, etc.).
2. Provide clear and concise answers.
Example:
"We have several properties that match your criteria. Here are a few options: [Property 1 Details], [Property 2 Details], [Property 3 Details]. Would you like to schedule a viewing or need more information on any of these?"

General Tips:
1. Maintain a friendly and professional tone throughout the conversation.
2. Be concise and to the point, avoiding overly technical jargon.
3. Ensure quick and accurate responses to user queries.
4. Offer personalized assistance based on user inputs.
5. Ensure user data privacy and confidentiality at all times.

You will be provided with additional documents containing information on properties. Use these documents to answer questions based on the provided data.  Distance is calculated from the location of the house to the city center.
If the document does not contain the information needed to answer a question, simply write:
"T√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ v·∫•n ƒë·ªÅ n√†y."
"""


def first_conversational_rag(session_id):
    conversation_history[session_id] = {
        "messages": [{"role": "system", "content": system_prompt}],
        "info": {},
    }


def conversational_rag(session_id, question, history):
    limit = 3750
    # res = openai.Embedding.create(
    #     input=[question],
    #     engine="text-embedding-ada-002"
    # )
    # xq = res['data'][0]['embedding']
    tokenizer_sent = [tokenize(question)]
    xq = model.encode(tokenizer_sent)
    xq = xq.tolist()[0]
    contexts = []
    time_waited = 0
    while len(contexts) < 3 and time_waited < 60 * 5:
        res = index.query(vector=xq, top_k=3, include_metadata=True)
        contexts = contexts + [x["metadata"]["text"] for x in res["matches"]]
        print(f"Retrieved {contexts}")
        time.sleep(2)
        time_waited += 20
    if time_waited >= 60 * 5:
        print("Timed out waiting for contexts to be retrieved.")
        contexts = ["No documents retrieved. Try to answer the question yourself!"]
    for i in range(1, len(contexts)):
        if len("\n".join(contexts[:i])) >= limit:
            prompt = "\n".join(contexts[: i - 1])
            break
        elif i == len(contexts) - 1:
            prompt = "\n".join(contexts)
    # Include stored information in the context
    stored_history = conversation_history[session_id]["messages"]
    stored_history.append(
        {"role": "system", "content": "Additional document:" + prompt}
    )
    stored_history.append({"role": "user", "content": question})

    response = client.chat.completions.create(
        model="gpt-3.5-turbo", messages=stored_history, max_tokens=350, temperature=0.7
    )
    answer = response.choices[0].message.content
    conversation_history[session_id]["messages"].append(
        {"role": "user", "content": question}
    )
    conversation_history[session_id]["messages"].append(
        {"role": "assistant", "content": answer}
    )
    history.append((question, answer))
    return "", history



def clear_history(session_id):
    if session_id in conversation_history:
        del conversation_history[session_id]
    first_conversational_rag(session_id)
    gr.Info("X√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán.")
    return


def open_ui(session_id):
    if session_id not in conversation_history:
        first_conversational_rag(session_id)
        return gr.update(visible=True)
    return None, None

def chatbot_interface():
    with gr.Blocks() as demo:
        gr.Markdown("""
        <style>
            body {font-family: Arial, sans-serif;}
            h1 {color: #4CAF50;}
            .gradio-container {background-color: #f9f9f9; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);}
            .gradio-row {margin-bottom: 10px;}
        </style>
        """)
        gr.Markdown("<h1 style='text-align: center;'>ü§ñ Chatbot</h1>")
        gr.Image("assets/pexels-binyaminmellish-1396132.jpg", elem_id="real-estate-image", show_label=False, interactive=False)
        
        session_id = gr.Textbox(label="Nh·∫≠p v√†o ƒë√¢y t√™n ƒëo·∫°n h·ªôi tho·∫°i:")
        start_button = gr.Button("B·∫Øt ƒë·∫ßu cu·ªôc h·ªôi tho·∫°i", variant="primary")

        with gr.Column(visible=False) as main_row:
            chatbot = gr.Chatbot(
                value=[
                    [
                        None,
                        "Xin ch√†o, Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi c√¥ng ty b·∫•t ƒë·ªông s·∫£n $mart Home. T√¥i l√† tr·ª£ l√Ω ·∫£o ·ªü ƒë√¢y ƒë·ªÉ gi√∫p b·∫°n gi·∫£i quy·∫øt c√°c nhu c·∫ßu v·ªÅ b·∫•t ƒë·ªông s·∫£n. H√¥m nay t√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n nh∆∞ th·∫ø n√†o?",
                    ]
                ],
                label="Chatbot",
                placeholder="Chatbot is ready to answer your questions.",
            )
            question = gr.Textbox(label="C√¢u h·ªèi c·ªßa b·∫°n")
            # submit_btn = gr.Button("Submit")
            clear_btn = gr.Button("X√≥a l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán")

            question.submit(
                conversational_rag,
                inputs=[session_id, question, chatbot],
                outputs=[question, chatbot],
            )
            clear_btn.click(clear_history, inputs=[session_id], outputs=[])

        start_button.click(open_ui, inputs=[session_id], outputs=[main_row])

# Main 
with gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.blue, secondary_hue=gr.themes.colors.sky)) as main:
    
    with gr.Tabs():
        
        with gr.TabItem("üè† Home"):
            home()
            
        with gr.TabItem("üí∞ Price Prediction"):
            predict_interface()
            
        with gr.TabItem("ü§ñ Chatbot"):
            chatbot_interface()

main.launch(inbrowser=True)
